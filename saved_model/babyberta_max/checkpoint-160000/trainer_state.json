{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.196806392706603,
  "eval_steps": 500,
  "global_step": 160000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01,
      "grad_norm": 4.668549060821533,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 8.9663,
      "step": 500
    },
    {
      "epoch": 0.01,
      "grad_norm": 8.424199104309082,
      "learning_rate": 4.166666666666667e-06,
      "loss": 8.543,
      "step": 1000
    },
    {
      "epoch": 0.02,
      "grad_norm": 3.6586430072784424,
      "learning_rate": 6.25e-06,
      "loss": 8.0633,
      "step": 1500
    },
    {
      "epoch": 0.03,
      "grad_norm": 4.000547885894775,
      "learning_rate": 8.333333333333334e-06,
      "loss": 7.5949,
      "step": 2000
    },
    {
      "epoch": 0.03,
      "grad_norm": 4.072261810302734,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 7.1446,
      "step": 2500
    },
    {
      "epoch": 0.04,
      "grad_norm": 4.2805376052856445,
      "learning_rate": 1.25e-05,
      "loss": 6.6688,
      "step": 3000
    },
    {
      "epoch": 0.05,
      "grad_norm": 5.013428688049316,
      "learning_rate": 1.4583333333333335e-05,
      "loss": 6.2874,
      "step": 3500
    },
    {
      "epoch": 0.05,
      "grad_norm": 3.732081413269043,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 6.0067,
      "step": 4000
    },
    {
      "epoch": 0.06,
      "grad_norm": 3.6859891414642334,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 5.8178,
      "step": 4500
    },
    {
      "epoch": 0.07,
      "grad_norm": 6.3175153732299805,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 5.7527,
      "step": 5000
    },
    {
      "epoch": 0.08,
      "grad_norm": 4.888040542602539,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 5.6341,
      "step": 5500
    },
    {
      "epoch": 0.08,
      "grad_norm": 3.454169273376465,
      "learning_rate": 2.5e-05,
      "loss": 5.5526,
      "step": 6000
    },
    {
      "epoch": 0.09,
      "grad_norm": 4.4149346351623535,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 5.4667,
      "step": 6500
    },
    {
      "epoch": 0.1,
      "grad_norm": 4.268485069274902,
      "learning_rate": 2.916666666666667e-05,
      "loss": 5.3667,
      "step": 7000
    },
    {
      "epoch": 0.1,
      "grad_norm": 4.9813079833984375,
      "learning_rate": 3.125e-05,
      "loss": 5.2817,
      "step": 7500
    },
    {
      "epoch": 0.11,
      "grad_norm": 6.044540882110596,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 5.2482,
      "step": 8000
    },
    {
      "epoch": 0.12,
      "grad_norm": 8.18006420135498,
      "learning_rate": 3.541666666666667e-05,
      "loss": 5.2248,
      "step": 8500
    },
    {
      "epoch": 0.12,
      "grad_norm": 5.105822563171387,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 5.157,
      "step": 9000
    },
    {
      "epoch": 0.13,
      "grad_norm": 7.323507308959961,
      "learning_rate": 3.958333333333333e-05,
      "loss": 5.1009,
      "step": 9500
    },
    {
      "epoch": 0.14,
      "grad_norm": 4.774269104003906,
      "learning_rate": 4.166666666666667e-05,
      "loss": 5.1593,
      "step": 10000
    },
    {
      "epoch": 0.14,
      "grad_norm": 5.510690212249756,
      "learning_rate": 4.375e-05,
      "loss": 5.0667,
      "step": 10500
    },
    {
      "epoch": 0.15,
      "grad_norm": 6.035452842712402,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 5.0017,
      "step": 11000
    },
    {
      "epoch": 0.16,
      "grad_norm": 4.136521816253662,
      "learning_rate": 4.791666666666667e-05,
      "loss": 4.9185,
      "step": 11500
    },
    {
      "epoch": 0.16,
      "grad_norm": 5.626830101013184,
      "learning_rate": 5e-05,
      "loss": 4.9126,
      "step": 12000
    },
    {
      "epoch": 0.17,
      "grad_norm": 7.2962493896484375,
      "learning_rate": 5.208333333333334e-05,
      "loss": 4.9094,
      "step": 12500
    },
    {
      "epoch": 0.18,
      "grad_norm": 6.355668067932129,
      "learning_rate": 5.4166666666666664e-05,
      "loss": 4.794,
      "step": 13000
    },
    {
      "epoch": 0.19,
      "grad_norm": 5.2980451583862305,
      "learning_rate": 5.6250000000000005e-05,
      "loss": 4.8102,
      "step": 13500
    },
    {
      "epoch": 0.19,
      "grad_norm": 8.795838356018066,
      "learning_rate": 5.833333333333334e-05,
      "loss": 4.7045,
      "step": 14000
    },
    {
      "epoch": 0.2,
      "grad_norm": 8.64780044555664,
      "learning_rate": 6.041666666666667e-05,
      "loss": 4.7122,
      "step": 14500
    },
    {
      "epoch": 0.21,
      "grad_norm": 5.897796630859375,
      "learning_rate": 6.25e-05,
      "loss": 4.7207,
      "step": 15000
    },
    {
      "epoch": 0.21,
      "grad_norm": 7.543278694152832,
      "learning_rate": 6.458333333333334e-05,
      "loss": 4.6243,
      "step": 15500
    },
    {
      "epoch": 0.22,
      "grad_norm": 8.449273109436035,
      "learning_rate": 6.666666666666667e-05,
      "loss": 4.6785,
      "step": 16000
    },
    {
      "epoch": 0.23,
      "grad_norm": 6.3780598640441895,
      "learning_rate": 6.875e-05,
      "loss": 4.5437,
      "step": 16500
    },
    {
      "epoch": 0.23,
      "grad_norm": 6.095524311065674,
      "learning_rate": 7.083333333333334e-05,
      "loss": 4.4663,
      "step": 17000
    },
    {
      "epoch": 0.24,
      "grad_norm": 9.121079444885254,
      "learning_rate": 7.291666666666667e-05,
      "loss": 4.4433,
      "step": 17500
    },
    {
      "epoch": 0.25,
      "grad_norm": 10.38794994354248,
      "learning_rate": 7.500000000000001e-05,
      "loss": 4.4671,
      "step": 18000
    },
    {
      "epoch": 0.25,
      "grad_norm": 8.8768949508667,
      "learning_rate": 7.708333333333334e-05,
      "loss": 4.3599,
      "step": 18500
    },
    {
      "epoch": 0.26,
      "grad_norm": 6.399158477783203,
      "learning_rate": 7.916666666666666e-05,
      "loss": 4.3233,
      "step": 19000
    },
    {
      "epoch": 0.27,
      "grad_norm": 6.970088958740234,
      "learning_rate": 8.125000000000001e-05,
      "loss": 4.3198,
      "step": 19500
    },
    {
      "epoch": 0.27,
      "grad_norm": 8.895404815673828,
      "learning_rate": 8.333333333333334e-05,
      "loss": 4.2622,
      "step": 20000
    },
    {
      "epoch": 0.28,
      "grad_norm": 8.083784103393555,
      "learning_rate": 8.541666666666666e-05,
      "loss": 4.321,
      "step": 20500
    },
    {
      "epoch": 0.29,
      "grad_norm": 5.287428855895996,
      "learning_rate": 8.75e-05,
      "loss": 4.2689,
      "step": 21000
    },
    {
      "epoch": 0.3,
      "grad_norm": 6.525877952575684,
      "learning_rate": 8.958333333333335e-05,
      "loss": 4.1399,
      "step": 21500
    },
    {
      "epoch": 0.3,
      "grad_norm": 13.572369575500488,
      "learning_rate": 9.166666666666667e-05,
      "loss": 4.1717,
      "step": 22000
    },
    {
      "epoch": 0.31,
      "grad_norm": 8.153557777404785,
      "learning_rate": 9.375e-05,
      "loss": 4.1275,
      "step": 22500
    },
    {
      "epoch": 0.32,
      "grad_norm": 6.667051792144775,
      "learning_rate": 9.583333333333334e-05,
      "loss": 4.0792,
      "step": 23000
    },
    {
      "epoch": 0.32,
      "grad_norm": 7.616429328918457,
      "learning_rate": 9.791666666666667e-05,
      "loss": 4.0678,
      "step": 23500
    },
    {
      "epoch": 0.33,
      "grad_norm": 7.82258415222168,
      "learning_rate": 0.0001,
      "loss": 4.0175,
      "step": 24000
    },
    {
      "epoch": 0.34,
      "grad_norm": 6.4678874015808105,
      "learning_rate": 9.963235294117647e-05,
      "loss": 3.977,
      "step": 24500
    },
    {
      "epoch": 0.34,
      "grad_norm": 8.62402057647705,
      "learning_rate": 9.926470588235295e-05,
      "loss": 4.026,
      "step": 25000
    },
    {
      "epoch": 0.35,
      "grad_norm": 9.723004341125488,
      "learning_rate": 9.889705882352942e-05,
      "loss": 3.9351,
      "step": 25500
    },
    {
      "epoch": 0.36,
      "grad_norm": 7.187439441680908,
      "learning_rate": 9.852941176470589e-05,
      "loss": 3.9105,
      "step": 26000
    },
    {
      "epoch": 0.36,
      "grad_norm": 7.146078109741211,
      "learning_rate": 9.816176470588235e-05,
      "loss": 3.7813,
      "step": 26500
    },
    {
      "epoch": 0.37,
      "grad_norm": 9.218223571777344,
      "learning_rate": 9.779411764705882e-05,
      "loss": 3.8997,
      "step": 27000
    },
    {
      "epoch": 0.38,
      "grad_norm": 5.036672115325928,
      "learning_rate": 9.74264705882353e-05,
      "loss": 3.8783,
      "step": 27500
    },
    {
      "epoch": 0.38,
      "grad_norm": 9.126553535461426,
      "learning_rate": 9.705882352941177e-05,
      "loss": 3.8484,
      "step": 28000
    },
    {
      "epoch": 0.39,
      "grad_norm": 7.859405517578125,
      "learning_rate": 9.669117647058824e-05,
      "loss": 3.8239,
      "step": 28500
    },
    {
      "epoch": 0.4,
      "grad_norm": 11.847914695739746,
      "learning_rate": 9.632352941176472e-05,
      "loss": 3.7648,
      "step": 29000
    },
    {
      "epoch": 0.41,
      "grad_norm": 9.544029235839844,
      "learning_rate": 9.595588235294119e-05,
      "loss": 3.7694,
      "step": 29500
    },
    {
      "epoch": 0.41,
      "grad_norm": 9.339783668518066,
      "learning_rate": 9.558823529411765e-05,
      "loss": 3.7464,
      "step": 30000
    },
    {
      "epoch": 0.42,
      "grad_norm": 7.0128278732299805,
      "learning_rate": 9.522058823529412e-05,
      "loss": 3.6628,
      "step": 30500
    },
    {
      "epoch": 0.43,
      "grad_norm": 5.961019039154053,
      "learning_rate": 9.485294117647059e-05,
      "loss": 3.6819,
      "step": 31000
    },
    {
      "epoch": 0.43,
      "grad_norm": 7.822605609893799,
      "learning_rate": 9.448529411764707e-05,
      "loss": 3.5968,
      "step": 31500
    },
    {
      "epoch": 0.44,
      "grad_norm": 8.252145767211914,
      "learning_rate": 9.411764705882353e-05,
      "loss": 3.6494,
      "step": 32000
    },
    {
      "epoch": 0.45,
      "grad_norm": 7.034939765930176,
      "learning_rate": 9.375e-05,
      "loss": 3.5776,
      "step": 32500
    },
    {
      "epoch": 0.45,
      "grad_norm": 7.956359386444092,
      "learning_rate": 9.338235294117648e-05,
      "loss": 3.6368,
      "step": 33000
    },
    {
      "epoch": 0.46,
      "grad_norm": 9.834518432617188,
      "learning_rate": 9.301470588235295e-05,
      "loss": 3.6306,
      "step": 33500
    },
    {
      "epoch": 0.47,
      "grad_norm": 8.403223037719727,
      "learning_rate": 9.264705882352942e-05,
      "loss": 3.5496,
      "step": 34000
    },
    {
      "epoch": 0.47,
      "grad_norm": 9.821874618530273,
      "learning_rate": 9.22794117647059e-05,
      "loss": 3.5886,
      "step": 34500
    },
    {
      "epoch": 0.48,
      "grad_norm": 9.683029174804688,
      "learning_rate": 9.191176470588235e-05,
      "loss": 3.5022,
      "step": 35000
    },
    {
      "epoch": 0.49,
      "grad_norm": 8.855743408203125,
      "learning_rate": 9.154411764705882e-05,
      "loss": 3.5853,
      "step": 35500
    },
    {
      "epoch": 0.49,
      "grad_norm": 6.865670680999756,
      "learning_rate": 9.11764705882353e-05,
      "loss": 3.5519,
      "step": 36000
    },
    {
      "epoch": 0.5,
      "grad_norm": 8.719995498657227,
      "learning_rate": 9.080882352941177e-05,
      "loss": 3.4959,
      "step": 36500
    },
    {
      "epoch": 0.51,
      "grad_norm": 7.665560722351074,
      "learning_rate": 9.044117647058823e-05,
      "loss": 3.5175,
      "step": 37000
    },
    {
      "epoch": 0.51,
      "grad_norm": 8.109221458435059,
      "learning_rate": 9.007352941176471e-05,
      "loss": 3.553,
      "step": 37500
    },
    {
      "epoch": 0.52,
      "grad_norm": 7.250734329223633,
      "learning_rate": 8.970588235294118e-05,
      "loss": 3.4786,
      "step": 38000
    },
    {
      "epoch": 0.53,
      "grad_norm": 9.503016471862793,
      "learning_rate": 8.933823529411765e-05,
      "loss": 3.4607,
      "step": 38500
    },
    {
      "epoch": 0.54,
      "grad_norm": 7.942354679107666,
      "learning_rate": 8.897058823529412e-05,
      "loss": 3.4316,
      "step": 39000
    },
    {
      "epoch": 0.54,
      "grad_norm": 8.728364944458008,
      "learning_rate": 8.860294117647058e-05,
      "loss": 3.4736,
      "step": 39500
    },
    {
      "epoch": 0.55,
      "grad_norm": 9.495680809020996,
      "learning_rate": 8.823529411764706e-05,
      "loss": 3.4478,
      "step": 40000
    },
    {
      "epoch": 0.56,
      "grad_norm": 9.203056335449219,
      "learning_rate": 8.786764705882353e-05,
      "loss": 3.4784,
      "step": 40500
    },
    {
      "epoch": 0.56,
      "grad_norm": 8.440266609191895,
      "learning_rate": 8.75e-05,
      "loss": 3.4173,
      "step": 41000
    },
    {
      "epoch": 0.57,
      "grad_norm": 8.030266761779785,
      "learning_rate": 8.713235294117648e-05,
      "loss": 3.3587,
      "step": 41500
    },
    {
      "epoch": 0.58,
      "grad_norm": 8.683430671691895,
      "learning_rate": 8.676470588235295e-05,
      "loss": 3.321,
      "step": 42000
    },
    {
      "epoch": 0.58,
      "grad_norm": 8.899645805358887,
      "learning_rate": 8.639705882352941e-05,
      "loss": 3.3073,
      "step": 42500
    },
    {
      "epoch": 0.59,
      "grad_norm": 7.755221366882324,
      "learning_rate": 8.60294117647059e-05,
      "loss": 3.3522,
      "step": 43000
    },
    {
      "epoch": 0.6,
      "grad_norm": 10.511100769042969,
      "learning_rate": 8.566176470588235e-05,
      "loss": 3.3712,
      "step": 43500
    },
    {
      "epoch": 0.6,
      "grad_norm": 8.996071815490723,
      "learning_rate": 8.529411764705883e-05,
      "loss": 3.3119,
      "step": 44000
    },
    {
      "epoch": 0.61,
      "grad_norm": 11.275046348571777,
      "learning_rate": 8.49264705882353e-05,
      "loss": 3.3053,
      "step": 44500
    },
    {
      "epoch": 0.62,
      "grad_norm": 6.041260242462158,
      "learning_rate": 8.455882352941176e-05,
      "loss": 3.3759,
      "step": 45000
    },
    {
      "epoch": 0.62,
      "grad_norm": 8.957889556884766,
      "learning_rate": 8.419117647058824e-05,
      "loss": 3.3279,
      "step": 45500
    },
    {
      "epoch": 0.63,
      "grad_norm": 6.893215656280518,
      "learning_rate": 8.382352941176471e-05,
      "loss": 3.3601,
      "step": 46000
    },
    {
      "epoch": 0.64,
      "grad_norm": 7.795255661010742,
      "learning_rate": 8.345588235294118e-05,
      "loss": 3.356,
      "step": 46500
    },
    {
      "epoch": 0.65,
      "grad_norm": 10.017663955688477,
      "learning_rate": 8.308823529411766e-05,
      "loss": 3.303,
      "step": 47000
    },
    {
      "epoch": 0.65,
      "grad_norm": 6.812861919403076,
      "learning_rate": 8.272058823529411e-05,
      "loss": 3.2898,
      "step": 47500
    },
    {
      "epoch": 0.66,
      "grad_norm": 9.101961135864258,
      "learning_rate": 8.23529411764706e-05,
      "loss": 3.3237,
      "step": 48000
    },
    {
      "epoch": 0.67,
      "grad_norm": 10.335956573486328,
      "learning_rate": 8.198529411764706e-05,
      "loss": 3.2211,
      "step": 48500
    },
    {
      "epoch": 0.67,
      "grad_norm": 7.268526077270508,
      "learning_rate": 8.161764705882353e-05,
      "loss": 3.2562,
      "step": 49000
    },
    {
      "epoch": 0.68,
      "grad_norm": 6.442749977111816,
      "learning_rate": 8.125000000000001e-05,
      "loss": 3.2102,
      "step": 49500
    },
    {
      "epoch": 0.69,
      "grad_norm": 8.960512161254883,
      "learning_rate": 8.088235294117648e-05,
      "loss": 3.177,
      "step": 50000
    },
    {
      "epoch": 0.69,
      "grad_norm": 9.41043758392334,
      "learning_rate": 8.051470588235294e-05,
      "loss": 3.1909,
      "step": 50500
    },
    {
      "epoch": 0.7,
      "grad_norm": 10.85272216796875,
      "learning_rate": 8.014705882352943e-05,
      "loss": 3.2597,
      "step": 51000
    },
    {
      "epoch": 0.71,
      "grad_norm": 9.452605247497559,
      "learning_rate": 7.977941176470589e-05,
      "loss": 3.2244,
      "step": 51500
    },
    {
      "epoch": 0.71,
      "grad_norm": 8.932177543640137,
      "learning_rate": 7.941176470588235e-05,
      "loss": 3.1765,
      "step": 52000
    },
    {
      "epoch": 0.72,
      "grad_norm": 10.128788948059082,
      "learning_rate": 7.904411764705883e-05,
      "loss": 3.2094,
      "step": 52500
    },
    {
      "epoch": 0.73,
      "grad_norm": 8.900965690612793,
      "learning_rate": 7.86764705882353e-05,
      "loss": 3.1658,
      "step": 53000
    },
    {
      "epoch": 0.73,
      "grad_norm": 9.674201011657715,
      "learning_rate": 7.830882352941176e-05,
      "loss": 3.0783,
      "step": 53500
    },
    {
      "epoch": 0.74,
      "grad_norm": 9.28171443939209,
      "learning_rate": 7.794117647058824e-05,
      "loss": 3.2256,
      "step": 54000
    },
    {
      "epoch": 0.75,
      "grad_norm": 9.59753704071045,
      "learning_rate": 7.757352941176471e-05,
      "loss": 3.1978,
      "step": 54500
    },
    {
      "epoch": 0.76,
      "grad_norm": 7.903644561767578,
      "learning_rate": 7.720588235294119e-05,
      "loss": 3.14,
      "step": 55000
    },
    {
      "epoch": 0.76,
      "grad_norm": 10.061162948608398,
      "learning_rate": 7.683823529411766e-05,
      "loss": 3.1534,
      "step": 55500
    },
    {
      "epoch": 0.77,
      "grad_norm": 9.795660018920898,
      "learning_rate": 7.647058823529411e-05,
      "loss": 3.1278,
      "step": 56000
    },
    {
      "epoch": 0.78,
      "grad_norm": 16.274402618408203,
      "learning_rate": 7.610294117647059e-05,
      "loss": 3.077,
      "step": 56500
    },
    {
      "epoch": 0.78,
      "grad_norm": 11.38890266418457,
      "learning_rate": 7.573529411764706e-05,
      "loss": 3.1454,
      "step": 57000
    },
    {
      "epoch": 0.79,
      "grad_norm": 8.325185775756836,
      "learning_rate": 7.536764705882353e-05,
      "loss": 3.0668,
      "step": 57500
    },
    {
      "epoch": 0.8,
      "grad_norm": 6.82019567489624,
      "learning_rate": 7.500000000000001e-05,
      "loss": 3.1041,
      "step": 58000
    },
    {
      "epoch": 0.8,
      "grad_norm": 10.946868896484375,
      "learning_rate": 7.463235294117647e-05,
      "loss": 3.0975,
      "step": 58500
    },
    {
      "epoch": 0.81,
      "grad_norm": 7.650588035583496,
      "learning_rate": 7.426470588235294e-05,
      "loss": 3.1199,
      "step": 59000
    },
    {
      "epoch": 0.82,
      "grad_norm": 10.694164276123047,
      "learning_rate": 7.389705882352942e-05,
      "loss": 3.1549,
      "step": 59500
    },
    {
      "epoch": 0.82,
      "grad_norm": 6.856054306030273,
      "learning_rate": 7.352941176470589e-05,
      "loss": 3.075,
      "step": 60000
    },
    {
      "epoch": 0.83,
      "grad_norm": 8.562009811401367,
      "learning_rate": 7.316176470588236e-05,
      "loss": 3.0968,
      "step": 60500
    },
    {
      "epoch": 0.84,
      "grad_norm": 13.419488906860352,
      "learning_rate": 7.279411764705882e-05,
      "loss": 3.1246,
      "step": 61000
    },
    {
      "epoch": 0.84,
      "grad_norm": 9.717684745788574,
      "learning_rate": 7.242647058823529e-05,
      "loss": 3.0777,
      "step": 61500
    },
    {
      "epoch": 0.85,
      "grad_norm": 9.839179039001465,
      "learning_rate": 7.205882352941177e-05,
      "loss": 3.1089,
      "step": 62000
    },
    {
      "epoch": 0.86,
      "grad_norm": 10.183613777160645,
      "learning_rate": 7.169117647058824e-05,
      "loss": 3.0422,
      "step": 62500
    },
    {
      "epoch": 0.86,
      "grad_norm": 9.26086139678955,
      "learning_rate": 7.13235294117647e-05,
      "loss": 3.0903,
      "step": 63000
    },
    {
      "epoch": 0.87,
      "grad_norm": 8.015355110168457,
      "learning_rate": 7.095588235294119e-05,
      "loss": 3.0684,
      "step": 63500
    },
    {
      "epoch": 0.88,
      "grad_norm": 10.462271690368652,
      "learning_rate": 7.058823529411765e-05,
      "loss": 3.0544,
      "step": 64000
    },
    {
      "epoch": 0.89,
      "grad_norm": 8.246779441833496,
      "learning_rate": 7.022058823529412e-05,
      "loss": 3.0057,
      "step": 64500
    },
    {
      "epoch": 0.89,
      "grad_norm": 8.835491180419922,
      "learning_rate": 6.985294117647059e-05,
      "loss": 3.0688,
      "step": 65000
    },
    {
      "epoch": 0.9,
      "grad_norm": 9.769129753112793,
      "learning_rate": 6.948529411764706e-05,
      "loss": 3.0678,
      "step": 65500
    },
    {
      "epoch": 0.91,
      "grad_norm": 6.452437877655029,
      "learning_rate": 6.911764705882354e-05,
      "loss": 2.9264,
      "step": 66000
    },
    {
      "epoch": 0.91,
      "grad_norm": 7.8726372718811035,
      "learning_rate": 6.875e-05,
      "loss": 3.0996,
      "step": 66500
    },
    {
      "epoch": 0.92,
      "grad_norm": 8.518866539001465,
      "learning_rate": 6.838235294117647e-05,
      "loss": 3.0461,
      "step": 67000
    },
    {
      "epoch": 0.93,
      "grad_norm": 9.046575546264648,
      "learning_rate": 6.801470588235295e-05,
      "loss": 3.0233,
      "step": 67500
    },
    {
      "epoch": 0.93,
      "grad_norm": 12.755053520202637,
      "learning_rate": 6.764705882352942e-05,
      "loss": 3.0289,
      "step": 68000
    },
    {
      "epoch": 0.94,
      "grad_norm": 8.1099853515625,
      "learning_rate": 6.727941176470589e-05,
      "loss": 3.0683,
      "step": 68500
    },
    {
      "epoch": 0.95,
      "grad_norm": 8.223502159118652,
      "learning_rate": 6.691176470588235e-05,
      "loss": 2.9861,
      "step": 69000
    },
    {
      "epoch": 0.95,
      "grad_norm": 7.771967887878418,
      "learning_rate": 6.654411764705882e-05,
      "loss": 3.002,
      "step": 69500
    },
    {
      "epoch": 0.96,
      "grad_norm": 7.5152716636657715,
      "learning_rate": 6.61764705882353e-05,
      "loss": 2.9341,
      "step": 70000
    },
    {
      "epoch": 0.97,
      "grad_norm": 8.840051651000977,
      "learning_rate": 6.580882352941177e-05,
      "loss": 3.0176,
      "step": 70500
    },
    {
      "epoch": 0.97,
      "grad_norm": 7.141811847686768,
      "learning_rate": 6.544117647058824e-05,
      "loss": 3.0787,
      "step": 71000
    },
    {
      "epoch": 0.98,
      "grad_norm": 9.0797119140625,
      "learning_rate": 6.507352941176472e-05,
      "loss": 2.9813,
      "step": 71500
    },
    {
      "epoch": 0.99,
      "grad_norm": 8.625663757324219,
      "learning_rate": 6.470588235294118e-05,
      "loss": 3.0085,
      "step": 72000
    },
    {
      "epoch": 1.0,
      "grad_norm": 7.973604202270508,
      "learning_rate": 6.433823529411765e-05,
      "loss": 2.9635,
      "step": 72500
    },
    {
      "epoch": 1.0,
      "grad_norm": 6.061384677886963,
      "learning_rate": 6.397058823529412e-05,
      "loss": 2.9021,
      "step": 73000
    },
    {
      "epoch": 1.01,
      "grad_norm": 8.697607040405273,
      "learning_rate": 6.360294117647059e-05,
      "loss": 2.9864,
      "step": 73500
    },
    {
      "epoch": 1.02,
      "grad_norm": 9.832098007202148,
      "learning_rate": 6.323529411764705e-05,
      "loss": 2.8896,
      "step": 74000
    },
    {
      "epoch": 1.02,
      "grad_norm": 9.650871276855469,
      "learning_rate": 6.286764705882353e-05,
      "loss": 2.9207,
      "step": 74500
    },
    {
      "epoch": 1.03,
      "grad_norm": 8.408977508544922,
      "learning_rate": 6.25e-05,
      "loss": 2.9293,
      "step": 75000
    },
    {
      "epoch": 1.04,
      "grad_norm": 12.543868064880371,
      "learning_rate": 6.213235294117647e-05,
      "loss": 2.9816,
      "step": 75500
    },
    {
      "epoch": 1.04,
      "grad_norm": 8.534830093383789,
      "learning_rate": 6.176470588235295e-05,
      "loss": 2.9491,
      "step": 76000
    },
    {
      "epoch": 1.05,
      "grad_norm": 9.656481742858887,
      "learning_rate": 6.139705882352942e-05,
      "loss": 2.9469,
      "step": 76500
    },
    {
      "epoch": 1.06,
      "grad_norm": 8.938689231872559,
      "learning_rate": 6.102941176470589e-05,
      "loss": 2.9042,
      "step": 77000
    },
    {
      "epoch": 1.06,
      "grad_norm": 8.528449058532715,
      "learning_rate": 6.066176470588235e-05,
      "loss": 2.9214,
      "step": 77500
    },
    {
      "epoch": 1.07,
      "grad_norm": 7.587057113647461,
      "learning_rate": 6.0294117647058825e-05,
      "loss": 2.9196,
      "step": 78000
    },
    {
      "epoch": 1.08,
      "grad_norm": 11.18380355834961,
      "learning_rate": 5.992647058823529e-05,
      "loss": 2.9462,
      "step": 78500
    },
    {
      "epoch": 1.08,
      "grad_norm": 10.14400863647461,
      "learning_rate": 5.9558823529411766e-05,
      "loss": 2.9225,
      "step": 79000
    },
    {
      "epoch": 1.09,
      "grad_norm": 9.842268943786621,
      "learning_rate": 5.919117647058824e-05,
      "loss": 2.8883,
      "step": 79500
    },
    {
      "epoch": 1.1,
      "grad_norm": 8.287839889526367,
      "learning_rate": 5.882352941176471e-05,
      "loss": 2.8186,
      "step": 80000
    },
    {
      "epoch": 1.11,
      "grad_norm": 12.437474250793457,
      "learning_rate": 5.845588235294118e-05,
      "loss": 2.9166,
      "step": 80500
    },
    {
      "epoch": 1.11,
      "grad_norm": 10.386154174804688,
      "learning_rate": 5.8088235294117656e-05,
      "loss": 2.9248,
      "step": 81000
    },
    {
      "epoch": 1.12,
      "grad_norm": 9.03937816619873,
      "learning_rate": 5.7720588235294116e-05,
      "loss": 2.9118,
      "step": 81500
    },
    {
      "epoch": 1.13,
      "grad_norm": 9.326769828796387,
      "learning_rate": 5.735294117647059e-05,
      "loss": 2.8904,
      "step": 82000
    },
    {
      "epoch": 1.13,
      "grad_norm": 10.716509819030762,
      "learning_rate": 5.698529411764706e-05,
      "loss": 2.8229,
      "step": 82500
    },
    {
      "epoch": 1.14,
      "grad_norm": 8.661471366882324,
      "learning_rate": 5.661764705882353e-05,
      "loss": 2.8208,
      "step": 83000
    },
    {
      "epoch": 1.15,
      "grad_norm": 8.94312572479248,
      "learning_rate": 5.6250000000000005e-05,
      "loss": 2.8365,
      "step": 83500
    },
    {
      "epoch": 1.15,
      "grad_norm": 9.211189270019531,
      "learning_rate": 5.588235294117647e-05,
      "loss": 2.8418,
      "step": 84000
    },
    {
      "epoch": 1.16,
      "grad_norm": 7.4303765296936035,
      "learning_rate": 5.5514705882352946e-05,
      "loss": 2.8549,
      "step": 84500
    },
    {
      "epoch": 1.17,
      "grad_norm": 9.300121307373047,
      "learning_rate": 5.514705882352942e-05,
      "loss": 2.8631,
      "step": 85000
    },
    {
      "epoch": 1.17,
      "grad_norm": 10.78126335144043,
      "learning_rate": 5.477941176470589e-05,
      "loss": 2.891,
      "step": 85500
    },
    {
      "epoch": 1.18,
      "grad_norm": 11.934907913208008,
      "learning_rate": 5.441176470588235e-05,
      "loss": 2.8165,
      "step": 86000
    },
    {
      "epoch": 1.19,
      "grad_norm": 9.24839973449707,
      "learning_rate": 5.404411764705882e-05,
      "loss": 2.816,
      "step": 86500
    },
    {
      "epoch": 1.19,
      "grad_norm": 14.00192928314209,
      "learning_rate": 5.3676470588235296e-05,
      "loss": 2.8461,
      "step": 87000
    },
    {
      "epoch": 1.2,
      "grad_norm": 7.532716274261475,
      "learning_rate": 5.3308823529411763e-05,
      "loss": 2.8642,
      "step": 87500
    },
    {
      "epoch": 1.21,
      "grad_norm": 8.919601440429688,
      "learning_rate": 5.294117647058824e-05,
      "loss": 2.8693,
      "step": 88000
    },
    {
      "epoch": 1.22,
      "grad_norm": 8.193729400634766,
      "learning_rate": 5.257352941176471e-05,
      "loss": 2.8326,
      "step": 88500
    },
    {
      "epoch": 1.22,
      "grad_norm": 9.490242958068848,
      "learning_rate": 5.2205882352941185e-05,
      "loss": 2.8636,
      "step": 89000
    },
    {
      "epoch": 1.23,
      "grad_norm": 8.486854553222656,
      "learning_rate": 5.183823529411765e-05,
      "loss": 2.8567,
      "step": 89500
    },
    {
      "epoch": 1.24,
      "grad_norm": 9.746833801269531,
      "learning_rate": 5.147058823529411e-05,
      "loss": 2.8808,
      "step": 90000
    },
    {
      "epoch": 1.24,
      "grad_norm": 8.630634307861328,
      "learning_rate": 5.110294117647059e-05,
      "loss": 2.88,
      "step": 90500
    },
    {
      "epoch": 1.25,
      "grad_norm": 8.54227352142334,
      "learning_rate": 5.073529411764706e-05,
      "loss": 2.8506,
      "step": 91000
    },
    {
      "epoch": 1.26,
      "grad_norm": 11.448349952697754,
      "learning_rate": 5.036764705882353e-05,
      "loss": 2.8119,
      "step": 91500
    },
    {
      "epoch": 1.26,
      "grad_norm": 8.291029930114746,
      "learning_rate": 5e-05,
      "loss": 2.777,
      "step": 92000
    },
    {
      "epoch": 1.27,
      "grad_norm": 8.090721130371094,
      "learning_rate": 4.9632352941176476e-05,
      "loss": 2.8363,
      "step": 92500
    },
    {
      "epoch": 1.28,
      "grad_norm": 11.47823715209961,
      "learning_rate": 4.9264705882352944e-05,
      "loss": 2.8681,
      "step": 93000
    },
    {
      "epoch": 1.28,
      "grad_norm": 9.191943168640137,
      "learning_rate": 4.889705882352941e-05,
      "loss": 2.7889,
      "step": 93500
    },
    {
      "epoch": 1.29,
      "grad_norm": 12.457440376281738,
      "learning_rate": 4.8529411764705885e-05,
      "loss": 2.8418,
      "step": 94000
    },
    {
      "epoch": 1.3,
      "grad_norm": 10.61316967010498,
      "learning_rate": 4.816176470588236e-05,
      "loss": 2.8085,
      "step": 94500
    },
    {
      "epoch": 1.3,
      "grad_norm": 10.013175964355469,
      "learning_rate": 4.7794117647058826e-05,
      "loss": 2.7679,
      "step": 95000
    },
    {
      "epoch": 1.31,
      "grad_norm": 7.930704593658447,
      "learning_rate": 4.742647058823529e-05,
      "loss": 2.7612,
      "step": 95500
    },
    {
      "epoch": 1.32,
      "grad_norm": 13.595943450927734,
      "learning_rate": 4.705882352941177e-05,
      "loss": 2.7975,
      "step": 96000
    },
    {
      "epoch": 1.32,
      "grad_norm": 9.05898380279541,
      "learning_rate": 4.669117647058824e-05,
      "loss": 2.7999,
      "step": 96500
    },
    {
      "epoch": 1.33,
      "grad_norm": 8.395550727844238,
      "learning_rate": 4.632352941176471e-05,
      "loss": 2.7948,
      "step": 97000
    },
    {
      "epoch": 1.34,
      "grad_norm": 6.994872570037842,
      "learning_rate": 4.5955882352941176e-05,
      "loss": 2.8166,
      "step": 97500
    },
    {
      "epoch": 1.35,
      "grad_norm": 11.800802230834961,
      "learning_rate": 4.558823529411765e-05,
      "loss": 2.7638,
      "step": 98000
    },
    {
      "epoch": 1.35,
      "grad_norm": 11.740187644958496,
      "learning_rate": 4.522058823529412e-05,
      "loss": 2.7317,
      "step": 98500
    },
    {
      "epoch": 1.36,
      "grad_norm": 9.811923027038574,
      "learning_rate": 4.485294117647059e-05,
      "loss": 2.7729,
      "step": 99000
    },
    {
      "epoch": 1.37,
      "grad_norm": 9.570334434509277,
      "learning_rate": 4.448529411764706e-05,
      "loss": 2.7382,
      "step": 99500
    },
    {
      "epoch": 1.37,
      "grad_norm": 9.44823932647705,
      "learning_rate": 4.411764705882353e-05,
      "loss": 2.8378,
      "step": 100000
    },
    {
      "epoch": 1.38,
      "grad_norm": 7.148866653442383,
      "learning_rate": 4.375e-05,
      "loss": 2.8278,
      "step": 100500
    },
    {
      "epoch": 1.39,
      "grad_norm": 9.79702377319336,
      "learning_rate": 4.3382352941176474e-05,
      "loss": 2.7717,
      "step": 101000
    },
    {
      "epoch": 1.39,
      "grad_norm": 10.741153717041016,
      "learning_rate": 4.301470588235295e-05,
      "loss": 2.812,
      "step": 101500
    },
    {
      "epoch": 1.4,
      "grad_norm": 12.766439437866211,
      "learning_rate": 4.2647058823529415e-05,
      "loss": 2.794,
      "step": 102000
    },
    {
      "epoch": 1.41,
      "grad_norm": 8.412168502807617,
      "learning_rate": 4.227941176470588e-05,
      "loss": 2.7289,
      "step": 102500
    },
    {
      "epoch": 1.41,
      "grad_norm": 11.8922119140625,
      "learning_rate": 4.1911764705882356e-05,
      "loss": 2.8029,
      "step": 103000
    },
    {
      "epoch": 1.42,
      "grad_norm": 10.464661598205566,
      "learning_rate": 4.154411764705883e-05,
      "loss": 2.7818,
      "step": 103500
    },
    {
      "epoch": 1.43,
      "grad_norm": 11.059032440185547,
      "learning_rate": 4.11764705882353e-05,
      "loss": 2.7649,
      "step": 104000
    },
    {
      "epoch": 1.43,
      "grad_norm": 9.024385452270508,
      "learning_rate": 4.0808823529411765e-05,
      "loss": 2.7365,
      "step": 104500
    },
    {
      "epoch": 1.44,
      "grad_norm": 9.3290433883667,
      "learning_rate": 4.044117647058824e-05,
      "loss": 2.7746,
      "step": 105000
    },
    {
      "epoch": 1.45,
      "grad_norm": 9.293534278869629,
      "learning_rate": 4.007352941176471e-05,
      "loss": 2.7289,
      "step": 105500
    },
    {
      "epoch": 1.46,
      "grad_norm": 7.470954418182373,
      "learning_rate": 3.970588235294117e-05,
      "loss": 2.7321,
      "step": 106000
    },
    {
      "epoch": 1.46,
      "grad_norm": 7.873173713684082,
      "learning_rate": 3.933823529411765e-05,
      "loss": 2.7818,
      "step": 106500
    },
    {
      "epoch": 1.47,
      "grad_norm": 8.36824893951416,
      "learning_rate": 3.897058823529412e-05,
      "loss": 2.7296,
      "step": 107000
    },
    {
      "epoch": 1.48,
      "grad_norm": 9.684273719787598,
      "learning_rate": 3.8602941176470595e-05,
      "loss": 2.7162,
      "step": 107500
    },
    {
      "epoch": 1.48,
      "grad_norm": 10.934491157531738,
      "learning_rate": 3.8235294117647055e-05,
      "loss": 2.7561,
      "step": 108000
    },
    {
      "epoch": 1.49,
      "grad_norm": 8.562397956848145,
      "learning_rate": 3.786764705882353e-05,
      "loss": 2.7824,
      "step": 108500
    },
    {
      "epoch": 1.5,
      "grad_norm": 11.922879219055176,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 2.7803,
      "step": 109000
    },
    {
      "epoch": 1.5,
      "grad_norm": 11.634876251220703,
      "learning_rate": 3.713235294117647e-05,
      "loss": 2.7291,
      "step": 109500
    },
    {
      "epoch": 1.51,
      "grad_norm": 8.709728240966797,
      "learning_rate": 3.6764705882352945e-05,
      "loss": 2.7904,
      "step": 110000
    },
    {
      "epoch": 1.52,
      "grad_norm": 8.2190580368042,
      "learning_rate": 3.639705882352941e-05,
      "loss": 2.7838,
      "step": 110500
    },
    {
      "epoch": 1.52,
      "grad_norm": 12.226909637451172,
      "learning_rate": 3.6029411764705886e-05,
      "loss": 2.7072,
      "step": 111000
    },
    {
      "epoch": 1.53,
      "grad_norm": 9.1654052734375,
      "learning_rate": 3.566176470588235e-05,
      "loss": 2.7981,
      "step": 111500
    },
    {
      "epoch": 1.54,
      "grad_norm": 10.961835861206055,
      "learning_rate": 3.529411764705883e-05,
      "loss": 2.7229,
      "step": 112000
    },
    {
      "epoch": 1.54,
      "grad_norm": 10.521209716796875,
      "learning_rate": 3.4926470588235294e-05,
      "loss": 2.7142,
      "step": 112500
    },
    {
      "epoch": 1.55,
      "grad_norm": 7.593011856079102,
      "learning_rate": 3.455882352941177e-05,
      "loss": 2.7037,
      "step": 113000
    },
    {
      "epoch": 1.56,
      "grad_norm": 10.024345397949219,
      "learning_rate": 3.4191176470588236e-05,
      "loss": 2.7332,
      "step": 113500
    },
    {
      "epoch": 1.57,
      "grad_norm": 7.786306858062744,
      "learning_rate": 3.382352941176471e-05,
      "loss": 2.7149,
      "step": 114000
    },
    {
      "epoch": 1.57,
      "grad_norm": 8.389938354492188,
      "learning_rate": 3.345588235294118e-05,
      "loss": 2.6648,
      "step": 114500
    },
    {
      "epoch": 1.58,
      "grad_norm": 9.963726997375488,
      "learning_rate": 3.308823529411765e-05,
      "loss": 2.757,
      "step": 115000
    },
    {
      "epoch": 1.59,
      "grad_norm": 7.804518222808838,
      "learning_rate": 3.272058823529412e-05,
      "loss": 2.6776,
      "step": 115500
    },
    {
      "epoch": 1.59,
      "grad_norm": 9.276665687561035,
      "learning_rate": 3.235294117647059e-05,
      "loss": 2.7138,
      "step": 116000
    },
    {
      "epoch": 1.6,
      "grad_norm": 9.688435554504395,
      "learning_rate": 3.198529411764706e-05,
      "loss": 2.7447,
      "step": 116500
    },
    {
      "epoch": 1.61,
      "grad_norm": 10.900594711303711,
      "learning_rate": 3.161764705882353e-05,
      "loss": 2.6758,
      "step": 117000
    },
    {
      "epoch": 1.61,
      "grad_norm": 10.061424255371094,
      "learning_rate": 3.125e-05,
      "loss": 2.7601,
      "step": 117500
    },
    {
      "epoch": 1.62,
      "grad_norm": 8.028416633605957,
      "learning_rate": 3.0882352941176475e-05,
      "loss": 2.7159,
      "step": 118000
    },
    {
      "epoch": 1.63,
      "grad_norm": 8.437253952026367,
      "learning_rate": 3.0514705882352945e-05,
      "loss": 2.7187,
      "step": 118500
    },
    {
      "epoch": 1.63,
      "grad_norm": 9.201059341430664,
      "learning_rate": 3.0147058823529413e-05,
      "loss": 2.7466,
      "step": 119000
    },
    {
      "epoch": 1.64,
      "grad_norm": 9.705693244934082,
      "learning_rate": 2.9779411764705883e-05,
      "loss": 2.7505,
      "step": 119500
    },
    {
      "epoch": 1.65,
      "grad_norm": 8.34201431274414,
      "learning_rate": 2.9411764705882354e-05,
      "loss": 2.6852,
      "step": 120000
    },
    {
      "epoch": 1.65,
      "grad_norm": 7.886158466339111,
      "learning_rate": 2.9044117647058828e-05,
      "loss": 2.6765,
      "step": 120500
    },
    {
      "epoch": 1.66,
      "grad_norm": 11.049752235412598,
      "learning_rate": 2.8676470588235295e-05,
      "loss": 2.6694,
      "step": 121000
    },
    {
      "epoch": 1.67,
      "grad_norm": 9.766443252563477,
      "learning_rate": 2.8308823529411766e-05,
      "loss": 2.6665,
      "step": 121500
    },
    {
      "epoch": 1.68,
      "grad_norm": 13.71064567565918,
      "learning_rate": 2.7941176470588236e-05,
      "loss": 2.6373,
      "step": 122000
    },
    {
      "epoch": 1.68,
      "grad_norm": 14.81066608428955,
      "learning_rate": 2.757352941176471e-05,
      "loss": 2.6612,
      "step": 122500
    },
    {
      "epoch": 1.69,
      "grad_norm": 11.846977233886719,
      "learning_rate": 2.7205882352941174e-05,
      "loss": 2.6891,
      "step": 123000
    },
    {
      "epoch": 1.7,
      "grad_norm": 10.691829681396484,
      "learning_rate": 2.6838235294117648e-05,
      "loss": 2.7496,
      "step": 123500
    },
    {
      "epoch": 1.7,
      "grad_norm": 9.36201000213623,
      "learning_rate": 2.647058823529412e-05,
      "loss": 2.6665,
      "step": 124000
    },
    {
      "epoch": 1.71,
      "grad_norm": 9.647175788879395,
      "learning_rate": 2.6102941176470593e-05,
      "loss": 2.7512,
      "step": 124500
    },
    {
      "epoch": 1.72,
      "grad_norm": 10.67611026763916,
      "learning_rate": 2.5735294117647057e-05,
      "loss": 2.6394,
      "step": 125000
    },
    {
      "epoch": 1.72,
      "grad_norm": 7.84030294418335,
      "learning_rate": 2.536764705882353e-05,
      "loss": 2.6072,
      "step": 125500
    },
    {
      "epoch": 1.73,
      "grad_norm": 9.035981178283691,
      "learning_rate": 2.5e-05,
      "loss": 2.6538,
      "step": 126000
    },
    {
      "epoch": 1.74,
      "grad_norm": 6.950472831726074,
      "learning_rate": 2.4632352941176472e-05,
      "loss": 2.6388,
      "step": 126500
    },
    {
      "epoch": 1.74,
      "grad_norm": 10.175054550170898,
      "learning_rate": 2.4264705882352942e-05,
      "loss": 2.6394,
      "step": 127000
    },
    {
      "epoch": 1.75,
      "grad_norm": 9.464038848876953,
      "learning_rate": 2.3897058823529413e-05,
      "loss": 2.6232,
      "step": 127500
    },
    {
      "epoch": 1.76,
      "grad_norm": 10.220008850097656,
      "learning_rate": 2.3529411764705884e-05,
      "loss": 2.6795,
      "step": 128000
    },
    {
      "epoch": 1.76,
      "grad_norm": 11.968875885009766,
      "learning_rate": 2.3161764705882354e-05,
      "loss": 2.6432,
      "step": 128500
    },
    {
      "epoch": 1.77,
      "grad_norm": 8.967157363891602,
      "learning_rate": 2.2794117647058825e-05,
      "loss": 2.6606,
      "step": 129000
    },
    {
      "epoch": 1.78,
      "grad_norm": 10.94906997680664,
      "learning_rate": 2.2426470588235296e-05,
      "loss": 2.6595,
      "step": 129500
    },
    {
      "epoch": 1.78,
      "grad_norm": 9.0806884765625,
      "learning_rate": 2.2058823529411766e-05,
      "loss": 2.7343,
      "step": 130000
    },
    {
      "epoch": 1.79,
      "grad_norm": 10.499659538269043,
      "learning_rate": 2.1691176470588237e-05,
      "loss": 2.7131,
      "step": 130500
    },
    {
      "epoch": 1.8,
      "grad_norm": 9.76312255859375,
      "learning_rate": 2.1323529411764707e-05,
      "loss": 2.6697,
      "step": 131000
    },
    {
      "epoch": 1.81,
      "grad_norm": 9.59325885772705,
      "learning_rate": 2.0955882352941178e-05,
      "loss": 2.6427,
      "step": 131500
    },
    {
      "epoch": 1.81,
      "grad_norm": 11.907631874084473,
      "learning_rate": 2.058823529411765e-05,
      "loss": 2.6596,
      "step": 132000
    },
    {
      "epoch": 1.82,
      "grad_norm": 6.7935261726379395,
      "learning_rate": 2.022058823529412e-05,
      "loss": 2.6156,
      "step": 132500
    },
    {
      "epoch": 1.83,
      "grad_norm": 7.962238311767578,
      "learning_rate": 1.9852941176470586e-05,
      "loss": 2.6548,
      "step": 133000
    },
    {
      "epoch": 1.83,
      "grad_norm": 8.786172866821289,
      "learning_rate": 1.948529411764706e-05,
      "loss": 2.6003,
      "step": 133500
    },
    {
      "epoch": 1.84,
      "grad_norm": 10.103988647460938,
      "learning_rate": 1.9117647058823528e-05,
      "loss": 2.7506,
      "step": 134000
    },
    {
      "epoch": 1.85,
      "grad_norm": 8.25961685180664,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 2.6502,
      "step": 134500
    },
    {
      "epoch": 1.85,
      "grad_norm": 9.232246398925781,
      "learning_rate": 1.8382352941176472e-05,
      "loss": 2.7205,
      "step": 135000
    },
    {
      "epoch": 1.86,
      "grad_norm": 9.368478775024414,
      "learning_rate": 1.8014705882352943e-05,
      "loss": 2.638,
      "step": 135500
    },
    {
      "epoch": 1.87,
      "grad_norm": 7.304430961608887,
      "learning_rate": 1.7647058823529414e-05,
      "loss": 2.645,
      "step": 136000
    },
    {
      "epoch": 1.87,
      "grad_norm": 6.221497058868408,
      "learning_rate": 1.7279411764705884e-05,
      "loss": 2.6473,
      "step": 136500
    },
    {
      "epoch": 1.88,
      "grad_norm": 8.032445907592773,
      "learning_rate": 1.6911764705882355e-05,
      "loss": 2.6777,
      "step": 137000
    },
    {
      "epoch": 1.89,
      "grad_norm": 12.908326148986816,
      "learning_rate": 1.6544117647058825e-05,
      "loss": 2.7099,
      "step": 137500
    },
    {
      "epoch": 1.89,
      "grad_norm": 10.064985275268555,
      "learning_rate": 1.6176470588235296e-05,
      "loss": 2.6545,
      "step": 138000
    },
    {
      "epoch": 1.9,
      "grad_norm": 9.55534839630127,
      "learning_rate": 1.5808823529411763e-05,
      "loss": 2.6014,
      "step": 138500
    },
    {
      "epoch": 1.91,
      "grad_norm": 10.834670066833496,
      "learning_rate": 1.5441176470588237e-05,
      "loss": 2.6137,
      "step": 139000
    },
    {
      "epoch": 1.92,
      "grad_norm": 9.537333488464355,
      "learning_rate": 1.5073529411764706e-05,
      "loss": 2.6325,
      "step": 139500
    },
    {
      "epoch": 1.92,
      "grad_norm": 8.793722152709961,
      "learning_rate": 1.4705882352941177e-05,
      "loss": 2.6417,
      "step": 140000
    },
    {
      "epoch": 1.93,
      "grad_norm": 10.981390953063965,
      "learning_rate": 1.4338235294117647e-05,
      "loss": 2.6895,
      "step": 140500
    },
    {
      "epoch": 1.94,
      "grad_norm": 8.050712585449219,
      "learning_rate": 1.3970588235294118e-05,
      "loss": 2.6478,
      "step": 141000
    },
    {
      "epoch": 1.94,
      "grad_norm": 9.864341735839844,
      "learning_rate": 1.3602941176470587e-05,
      "loss": 2.674,
      "step": 141500
    },
    {
      "epoch": 1.95,
      "grad_norm": 10.427000999450684,
      "learning_rate": 1.323529411764706e-05,
      "loss": 2.6455,
      "step": 142000
    },
    {
      "epoch": 1.96,
      "grad_norm": 7.18425178527832,
      "learning_rate": 1.2867647058823528e-05,
      "loss": 2.6376,
      "step": 142500
    },
    {
      "epoch": 1.96,
      "grad_norm": 9.625031471252441,
      "learning_rate": 1.25e-05,
      "loss": 2.5764,
      "step": 143000
    },
    {
      "epoch": 1.97,
      "grad_norm": 9.45374870300293,
      "learning_rate": 1.2132352941176471e-05,
      "loss": 2.6129,
      "step": 143500
    },
    {
      "epoch": 1.98,
      "grad_norm": 8.749260902404785,
      "learning_rate": 1.1764705882352942e-05,
      "loss": 2.6633,
      "step": 144000
    },
    {
      "epoch": 1.98,
      "grad_norm": 8.971302032470703,
      "learning_rate": 1.1397058823529412e-05,
      "loss": 2.6256,
      "step": 144500
    },
    {
      "epoch": 1.99,
      "grad_norm": 8.155241966247559,
      "learning_rate": 1.1029411764705883e-05,
      "loss": 2.5929,
      "step": 145000
    },
    {
      "epoch": 2.0,
      "grad_norm": 8.1375732421875,
      "learning_rate": 1.0661764705882354e-05,
      "loss": 2.5993,
      "step": 145500
    },
    {
      "epoch": 2.0,
      "grad_norm": 13.324559211730957,
      "learning_rate": 1.0294117647058824e-05,
      "loss": 2.5804,
      "step": 146000
    },
    {
      "epoch": 2.01,
      "grad_norm": 7.723830699920654,
      "learning_rate": 9.926470588235293e-06,
      "loss": 2.617,
      "step": 146500
    },
    {
      "epoch": 2.02,
      "grad_norm": 9.986788749694824,
      "learning_rate": 9.558823529411764e-06,
      "loss": 2.638,
      "step": 147000
    },
    {
      "epoch": 2.03,
      "grad_norm": 8.845501899719238,
      "learning_rate": 9.191176470588236e-06,
      "loss": 2.6452,
      "step": 147500
    },
    {
      "epoch": 2.03,
      "grad_norm": 8.044486045837402,
      "learning_rate": 8.823529411764707e-06,
      "loss": 2.6166,
      "step": 148000
    },
    {
      "epoch": 2.04,
      "grad_norm": 9.81928825378418,
      "learning_rate": 8.455882352941177e-06,
      "loss": 2.6099,
      "step": 148500
    },
    {
      "epoch": 2.05,
      "grad_norm": 13.456084251403809,
      "learning_rate": 8.088235294117648e-06,
      "loss": 2.6668,
      "step": 149000
    },
    {
      "epoch": 2.05,
      "grad_norm": 9.746840476989746,
      "learning_rate": 7.720588235294119e-06,
      "loss": 2.5999,
      "step": 149500
    },
    {
      "epoch": 2.06,
      "grad_norm": 10.062623977661133,
      "learning_rate": 7.3529411764705884e-06,
      "loss": 2.5937,
      "step": 150000
    },
    {
      "epoch": 2.07,
      "grad_norm": 7.619510650634766,
      "learning_rate": 6.985294117647059e-06,
      "loss": 2.6019,
      "step": 150500
    },
    {
      "epoch": 2.07,
      "grad_norm": 9.568086624145508,
      "learning_rate": 6.61764705882353e-06,
      "loss": 2.6553,
      "step": 151000
    },
    {
      "epoch": 2.08,
      "grad_norm": 9.451398849487305,
      "learning_rate": 6.25e-06,
      "loss": 2.5695,
      "step": 151500
    },
    {
      "epoch": 2.09,
      "grad_norm": 8.572153091430664,
      "learning_rate": 5.882352941176471e-06,
      "loss": 2.55,
      "step": 152000
    },
    {
      "epoch": 2.09,
      "grad_norm": 8.476541519165039,
      "learning_rate": 5.5147058823529415e-06,
      "loss": 2.5799,
      "step": 152500
    },
    {
      "epoch": 2.1,
      "grad_norm": 8.651778221130371,
      "learning_rate": 5.147058823529412e-06,
      "loss": 2.6107,
      "step": 153000
    },
    {
      "epoch": 2.11,
      "grad_norm": 8.576667785644531,
      "learning_rate": 4.779411764705882e-06,
      "loss": 2.6173,
      "step": 153500
    },
    {
      "epoch": 2.11,
      "grad_norm": 9.770224571228027,
      "learning_rate": 4.411764705882353e-06,
      "loss": 2.604,
      "step": 154000
    },
    {
      "epoch": 2.12,
      "grad_norm": 10.475117683410645,
      "learning_rate": 4.044117647058824e-06,
      "loss": 2.5906,
      "step": 154500
    },
    {
      "epoch": 2.13,
      "grad_norm": 9.225102424621582,
      "learning_rate": 3.6764705882352942e-06,
      "loss": 2.5448,
      "step": 155000
    },
    {
      "epoch": 2.14,
      "grad_norm": 8.609503746032715,
      "learning_rate": 3.308823529411765e-06,
      "loss": 2.6187,
      "step": 155500
    },
    {
      "epoch": 2.14,
      "grad_norm": 8.37807846069336,
      "learning_rate": 2.9411764705882355e-06,
      "loss": 2.5566,
      "step": 156000
    },
    {
      "epoch": 2.15,
      "grad_norm": 7.098818302154541,
      "learning_rate": 2.573529411764706e-06,
      "loss": 2.6212,
      "step": 156500
    },
    {
      "epoch": 2.16,
      "grad_norm": 9.952083587646484,
      "learning_rate": 2.2058823529411767e-06,
      "loss": 2.597,
      "step": 157000
    },
    {
      "epoch": 2.16,
      "grad_norm": 10.706490516662598,
      "learning_rate": 1.8382352941176471e-06,
      "loss": 2.5936,
      "step": 157500
    },
    {
      "epoch": 2.17,
      "grad_norm": 8.029232025146484,
      "learning_rate": 1.4705882352941177e-06,
      "loss": 2.5737,
      "step": 158000
    },
    {
      "epoch": 2.18,
      "grad_norm": 10.293347358703613,
      "learning_rate": 1.1029411764705884e-06,
      "loss": 2.6359,
      "step": 158500
    },
    {
      "epoch": 2.18,
      "grad_norm": 11.420540809631348,
      "learning_rate": 7.352941176470589e-07,
      "loss": 2.5978,
      "step": 159000
    },
    {
      "epoch": 2.19,
      "grad_norm": 10.009265899658203,
      "learning_rate": 3.6764705882352943e-07,
      "loss": 2.5913,
      "step": 159500
    },
    {
      "epoch": 2.2,
      "grad_norm": 12.160785675048828,
      "learning_rate": 0.0,
      "loss": 2.5947,
      "step": 160000
    }
  ],
  "logging_steps": 500,
  "max_steps": 160000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 40000,
  "total_flos": 1.2219087383473152e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
