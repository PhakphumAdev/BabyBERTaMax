{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0905333101543335,
  "eval_steps": 500,
  "global_step": 160000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 6.873701572418213,
      "learning_rate": 4.166666666666667e-06,
      "loss": 8.7858,
      "step": 1000
    },
    {
      "epoch": 0.04,
      "grad_norm": 3.9354796409606934,
      "learning_rate": 8.333333333333334e-06,
      "loss": 8.0223,
      "step": 2000
    },
    {
      "epoch": 0.06,
      "grad_norm": 3.5268523693084717,
      "learning_rate": 1.25e-05,
      "loss": 7.2753,
      "step": 3000
    },
    {
      "epoch": 0.08,
      "grad_norm": 4.033263206481934,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 6.7571,
      "step": 4000
    },
    {
      "epoch": 0.1,
      "grad_norm": 4.323718070983887,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 6.4964,
      "step": 5000
    },
    {
      "epoch": 0.12,
      "grad_norm": 4.536490440368652,
      "learning_rate": 2.5e-05,
      "loss": 6.296,
      "step": 6000
    },
    {
      "epoch": 0.14,
      "grad_norm": 7.590329647064209,
      "learning_rate": 2.916666666666667e-05,
      "loss": 6.1551,
      "step": 7000
    },
    {
      "epoch": 0.15,
      "grad_norm": 4.763405799865723,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 6.0241,
      "step": 8000
    },
    {
      "epoch": 0.17,
      "grad_norm": 6.47326135635376,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 5.9693,
      "step": 9000
    },
    {
      "epoch": 0.19,
      "grad_norm": 6.3245978355407715,
      "learning_rate": 4.166666666666667e-05,
      "loss": 5.8555,
      "step": 10000
    },
    {
      "epoch": 0.21,
      "grad_norm": 9.761412620544434,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 5.7556,
      "step": 11000
    },
    {
      "epoch": 0.23,
      "grad_norm": 7.545603275299072,
      "learning_rate": 5e-05,
      "loss": 5.6929,
      "step": 12000
    },
    {
      "epoch": 0.25,
      "grad_norm": 8.966048240661621,
      "learning_rate": 5.4166666666666664e-05,
      "loss": 5.5866,
      "step": 13000
    },
    {
      "epoch": 0.27,
      "grad_norm": 8.507040977478027,
      "learning_rate": 5.833333333333334e-05,
      "loss": 5.5067,
      "step": 14000
    },
    {
      "epoch": 0.29,
      "grad_norm": 5.296085834503174,
      "learning_rate": 6.25e-05,
      "loss": 5.4581,
      "step": 15000
    },
    {
      "epoch": 0.31,
      "grad_norm": 6.973036289215088,
      "learning_rate": 6.666666666666667e-05,
      "loss": 5.3387,
      "step": 16000
    },
    {
      "epoch": 0.33,
      "grad_norm": 6.518305778503418,
      "learning_rate": 7.083333333333334e-05,
      "loss": 5.2437,
      "step": 17000
    },
    {
      "epoch": 0.35,
      "grad_norm": 6.816214084625244,
      "learning_rate": 7.500000000000001e-05,
      "loss": 5.1388,
      "step": 18000
    },
    {
      "epoch": 0.37,
      "grad_norm": 6.172938823699951,
      "learning_rate": 7.916666666666666e-05,
      "loss": 5.0639,
      "step": 19000
    },
    {
      "epoch": 0.39,
      "grad_norm": 9.081149101257324,
      "learning_rate": 8.333333333333334e-05,
      "loss": 4.9769,
      "step": 20000
    },
    {
      "epoch": 0.41,
      "grad_norm": 7.243768215179443,
      "learning_rate": 8.75e-05,
      "loss": 4.9138,
      "step": 21000
    },
    {
      "epoch": 0.42,
      "grad_norm": 7.5572686195373535,
      "learning_rate": 9.166666666666667e-05,
      "loss": 4.8281,
      "step": 22000
    },
    {
      "epoch": 0.44,
      "grad_norm": 7.050774097442627,
      "learning_rate": 9.583333333333334e-05,
      "loss": 4.7736,
      "step": 23000
    },
    {
      "epoch": 0.46,
      "grad_norm": 8.406876564025879,
      "learning_rate": 0.0001,
      "loss": 4.6498,
      "step": 24000
    },
    {
      "epoch": 0.48,
      "grad_norm": 8.62806224822998,
      "learning_rate": 9.926470588235295e-05,
      "loss": 4.6613,
      "step": 25000
    },
    {
      "epoch": 0.5,
      "grad_norm": 6.305579662322998,
      "learning_rate": 9.852941176470589e-05,
      "loss": 4.5319,
      "step": 26000
    },
    {
      "epoch": 0.52,
      "grad_norm": 8.461843490600586,
      "learning_rate": 9.779411764705882e-05,
      "loss": 4.4778,
      "step": 27000
    },
    {
      "epoch": 0.54,
      "grad_norm": 9.962944984436035,
      "learning_rate": 9.705882352941177e-05,
      "loss": 4.4374,
      "step": 28000
    },
    {
      "epoch": 0.56,
      "grad_norm": 6.468562126159668,
      "learning_rate": 9.632352941176472e-05,
      "loss": 4.354,
      "step": 29000
    },
    {
      "epoch": 0.58,
      "grad_norm": 10.326207160949707,
      "learning_rate": 9.558823529411765e-05,
      "loss": 4.3319,
      "step": 30000
    },
    {
      "epoch": 0.6,
      "grad_norm": 5.904648780822754,
      "learning_rate": 9.485294117647059e-05,
      "loss": 4.2771,
      "step": 31000
    },
    {
      "epoch": 0.62,
      "grad_norm": 9.976485252380371,
      "learning_rate": 9.411764705882353e-05,
      "loss": 4.2891,
      "step": 32000
    },
    {
      "epoch": 0.64,
      "grad_norm": 6.569460868835449,
      "learning_rate": 9.338235294117648e-05,
      "loss": 4.208,
      "step": 33000
    },
    {
      "epoch": 0.66,
      "grad_norm": 10.485738754272461,
      "learning_rate": 9.264705882352942e-05,
      "loss": 4.1141,
      "step": 34000
    },
    {
      "epoch": 0.68,
      "grad_norm": 8.624751091003418,
      "learning_rate": 9.191176470588235e-05,
      "loss": 4.0954,
      "step": 35000
    },
    {
      "epoch": 0.7,
      "grad_norm": 9.734511375427246,
      "learning_rate": 9.11764705882353e-05,
      "loss": 4.0551,
      "step": 36000
    },
    {
      "epoch": 0.71,
      "grad_norm": 7.476958274841309,
      "learning_rate": 9.044117647058823e-05,
      "loss": 4.0657,
      "step": 37000
    },
    {
      "epoch": 0.73,
      "grad_norm": 8.53611946105957,
      "learning_rate": 8.970588235294118e-05,
      "loss": 3.9758,
      "step": 38000
    },
    {
      "epoch": 0.75,
      "grad_norm": 11.01172924041748,
      "learning_rate": 8.897058823529412e-05,
      "loss": 3.9874,
      "step": 39000
    },
    {
      "epoch": 0.77,
      "grad_norm": 9.987852096557617,
      "learning_rate": 8.823529411764706e-05,
      "loss": 3.9915,
      "step": 40000
    },
    {
      "epoch": 0.79,
      "grad_norm": 11.560639381408691,
      "learning_rate": 8.75e-05,
      "loss": 3.9193,
      "step": 41000
    },
    {
      "epoch": 0.81,
      "grad_norm": 12.169739723205566,
      "learning_rate": 8.676470588235295e-05,
      "loss": 3.906,
      "step": 42000
    },
    {
      "epoch": 0.83,
      "grad_norm": 9.812081336975098,
      "learning_rate": 8.60294117647059e-05,
      "loss": 3.8965,
      "step": 43000
    },
    {
      "epoch": 0.85,
      "grad_norm": 8.838102340698242,
      "learning_rate": 8.529411764705883e-05,
      "loss": 3.8441,
      "step": 44000
    },
    {
      "epoch": 0.87,
      "grad_norm": 8.244728088378906,
      "learning_rate": 8.455882352941176e-05,
      "loss": 3.815,
      "step": 45000
    },
    {
      "epoch": 0.89,
      "grad_norm": 7.41569709777832,
      "learning_rate": 8.382352941176471e-05,
      "loss": 3.7668,
      "step": 46000
    },
    {
      "epoch": 0.91,
      "grad_norm": 7.88485860824585,
      "learning_rate": 8.308823529411766e-05,
      "loss": 3.7808,
      "step": 47000
    },
    {
      "epoch": 0.93,
      "grad_norm": 7.628613471984863,
      "learning_rate": 8.23529411764706e-05,
      "loss": 3.7143,
      "step": 48000
    },
    {
      "epoch": 0.95,
      "grad_norm": 10.188919067382812,
      "learning_rate": 8.161764705882353e-05,
      "loss": 3.702,
      "step": 49000
    },
    {
      "epoch": 0.97,
      "grad_norm": 11.410271644592285,
      "learning_rate": 8.088235294117648e-05,
      "loss": 3.6665,
      "step": 50000
    },
    {
      "epoch": 0.99,
      "grad_norm": 8.860037803649902,
      "learning_rate": 8.014705882352943e-05,
      "loss": 3.6734,
      "step": 51000
    },
    {
      "epoch": 1.0,
      "grad_norm": 9.646566390991211,
      "learning_rate": 7.941176470588235e-05,
      "loss": 3.6611,
      "step": 52000
    },
    {
      "epoch": 1.02,
      "grad_norm": 11.08646297454834,
      "learning_rate": 7.86764705882353e-05,
      "loss": 3.6449,
      "step": 53000
    },
    {
      "epoch": 1.04,
      "grad_norm": 8.275884628295898,
      "learning_rate": 7.794117647058824e-05,
      "loss": 3.5757,
      "step": 54000
    },
    {
      "epoch": 1.06,
      "grad_norm": 9.152396202087402,
      "learning_rate": 7.720588235294119e-05,
      "loss": 3.595,
      "step": 55000
    },
    {
      "epoch": 1.08,
      "grad_norm": 8.296074867248535,
      "learning_rate": 7.647058823529411e-05,
      "loss": 3.5907,
      "step": 56000
    },
    {
      "epoch": 1.1,
      "grad_norm": 9.769944190979004,
      "learning_rate": 7.573529411764706e-05,
      "loss": 3.5137,
      "step": 57000
    },
    {
      "epoch": 1.12,
      "grad_norm": 9.187705039978027,
      "learning_rate": 7.500000000000001e-05,
      "loss": 3.5351,
      "step": 58000
    },
    {
      "epoch": 1.14,
      "grad_norm": 8.775904655456543,
      "learning_rate": 7.426470588235294e-05,
      "loss": 3.5337,
      "step": 59000
    },
    {
      "epoch": 1.16,
      "grad_norm": 10.101738929748535,
      "learning_rate": 7.352941176470589e-05,
      "loss": 3.5031,
      "step": 60000
    },
    {
      "epoch": 1.18,
      "grad_norm": 9.688544273376465,
      "learning_rate": 7.279411764705882e-05,
      "loss": 3.4926,
      "step": 61000
    },
    {
      "epoch": 1.2,
      "grad_norm": 11.671651840209961,
      "learning_rate": 7.205882352941177e-05,
      "loss": 3.4652,
      "step": 62000
    },
    {
      "epoch": 1.22,
      "grad_norm": 11.361359596252441,
      "learning_rate": 7.13235294117647e-05,
      "loss": 3.47,
      "step": 63000
    },
    {
      "epoch": 1.24,
      "grad_norm": 8.762004852294922,
      "learning_rate": 7.058823529411765e-05,
      "loss": 3.5051,
      "step": 64000
    },
    {
      "epoch": 1.26,
      "grad_norm": 8.019902229309082,
      "learning_rate": 6.985294117647059e-05,
      "loss": 3.4382,
      "step": 65000
    },
    {
      "epoch": 1.27,
      "grad_norm": 9.259806632995605,
      "learning_rate": 6.911764705882354e-05,
      "loss": 3.4621,
      "step": 66000
    },
    {
      "epoch": 1.29,
      "grad_norm": 9.017352104187012,
      "learning_rate": 6.838235294117647e-05,
      "loss": 3.4325,
      "step": 67000
    },
    {
      "epoch": 1.31,
      "grad_norm": 7.950279712677002,
      "learning_rate": 6.764705882352942e-05,
      "loss": 3.3947,
      "step": 68000
    },
    {
      "epoch": 1.33,
      "grad_norm": 8.935586929321289,
      "learning_rate": 6.691176470588235e-05,
      "loss": 3.4126,
      "step": 69000
    },
    {
      "epoch": 1.35,
      "grad_norm": 9.27876091003418,
      "learning_rate": 6.61764705882353e-05,
      "loss": 3.3704,
      "step": 70000
    },
    {
      "epoch": 1.37,
      "grad_norm": 10.46632194519043,
      "learning_rate": 6.544117647058824e-05,
      "loss": 3.3915,
      "step": 71000
    },
    {
      "epoch": 1.39,
      "grad_norm": 7.569766998291016,
      "learning_rate": 6.470588235294118e-05,
      "loss": 3.3488,
      "step": 72000
    },
    {
      "epoch": 1.41,
      "grad_norm": 11.28664779663086,
      "learning_rate": 6.397058823529412e-05,
      "loss": 3.3503,
      "step": 73000
    },
    {
      "epoch": 1.43,
      "grad_norm": 8.348357200622559,
      "learning_rate": 6.323529411764705e-05,
      "loss": 3.3724,
      "step": 74000
    },
    {
      "epoch": 1.45,
      "grad_norm": 9.337933540344238,
      "learning_rate": 6.25e-05,
      "loss": 3.3682,
      "step": 75000
    },
    {
      "epoch": 1.47,
      "grad_norm": 9.311748504638672,
      "learning_rate": 6.176470588235295e-05,
      "loss": 3.3101,
      "step": 76000
    },
    {
      "epoch": 1.49,
      "grad_norm": 8.727263450622559,
      "learning_rate": 6.102941176470589e-05,
      "loss": 3.3259,
      "step": 77000
    },
    {
      "epoch": 1.51,
      "grad_norm": 8.444270133972168,
      "learning_rate": 6.0294117647058825e-05,
      "loss": 3.3278,
      "step": 78000
    },
    {
      "epoch": 1.53,
      "grad_norm": 9.227408409118652,
      "learning_rate": 5.9558823529411766e-05,
      "loss": 3.3219,
      "step": 79000
    },
    {
      "epoch": 1.55,
      "grad_norm": 7.776394367218018,
      "learning_rate": 5.882352941176471e-05,
      "loss": 3.3151,
      "step": 80000
    },
    {
      "epoch": 1.56,
      "grad_norm": 8.084877967834473,
      "learning_rate": 5.8088235294117656e-05,
      "loss": 3.2811,
      "step": 81000
    },
    {
      "epoch": 1.58,
      "grad_norm": 11.58346939086914,
      "learning_rate": 5.735294117647059e-05,
      "loss": 3.3041,
      "step": 82000
    },
    {
      "epoch": 1.6,
      "grad_norm": 8.29504108428955,
      "learning_rate": 5.661764705882353e-05,
      "loss": 3.2885,
      "step": 83000
    },
    {
      "epoch": 1.62,
      "grad_norm": 9.67781925201416,
      "learning_rate": 5.588235294117647e-05,
      "loss": 3.2937,
      "step": 84000
    },
    {
      "epoch": 1.64,
      "grad_norm": 8.284786224365234,
      "learning_rate": 5.514705882352942e-05,
      "loss": 3.2316,
      "step": 85000
    },
    {
      "epoch": 1.66,
      "grad_norm": 8.04886245727539,
      "learning_rate": 5.441176470588235e-05,
      "loss": 3.2975,
      "step": 86000
    },
    {
      "epoch": 1.68,
      "grad_norm": 15.490123748779297,
      "learning_rate": 5.3676470588235296e-05,
      "loss": 3.2522,
      "step": 87000
    },
    {
      "epoch": 1.7,
      "grad_norm": 9.853570938110352,
      "learning_rate": 5.294117647058824e-05,
      "loss": 3.2501,
      "step": 88000
    },
    {
      "epoch": 1.72,
      "grad_norm": 9.256571769714355,
      "learning_rate": 5.2205882352941185e-05,
      "loss": 3.2419,
      "step": 89000
    },
    {
      "epoch": 1.74,
      "grad_norm": 7.940462589263916,
      "learning_rate": 5.147058823529411e-05,
      "loss": 3.236,
      "step": 90000
    },
    {
      "epoch": 1.76,
      "grad_norm": 10.107192993164062,
      "learning_rate": 5.073529411764706e-05,
      "loss": 3.2318,
      "step": 91000
    },
    {
      "epoch": 1.78,
      "grad_norm": 9.06688404083252,
      "learning_rate": 5e-05,
      "loss": 3.2316,
      "step": 92000
    },
    {
      "epoch": 1.8,
      "grad_norm": 8.728392601013184,
      "learning_rate": 4.9264705882352944e-05,
      "loss": 3.2238,
      "step": 93000
    },
    {
      "epoch": 1.82,
      "grad_norm": 7.162788391113281,
      "learning_rate": 4.8529411764705885e-05,
      "loss": 3.1952,
      "step": 94000
    },
    {
      "epoch": 1.84,
      "grad_norm": 12.205490112304688,
      "learning_rate": 4.7794117647058826e-05,
      "loss": 3.1878,
      "step": 95000
    },
    {
      "epoch": 1.85,
      "grad_norm": 14.291247367858887,
      "learning_rate": 4.705882352941177e-05,
      "loss": 3.1937,
      "step": 96000
    },
    {
      "epoch": 1.87,
      "grad_norm": 9.507146835327148,
      "learning_rate": 4.632352941176471e-05,
      "loss": 3.196,
      "step": 97000
    },
    {
      "epoch": 1.89,
      "grad_norm": 12.535651206970215,
      "learning_rate": 4.558823529411765e-05,
      "loss": 3.2216,
      "step": 98000
    },
    {
      "epoch": 1.91,
      "grad_norm": 8.675278663635254,
      "learning_rate": 4.485294117647059e-05,
      "loss": 3.2061,
      "step": 99000
    },
    {
      "epoch": 1.93,
      "grad_norm": 7.716191291809082,
      "learning_rate": 4.411764705882353e-05,
      "loss": 3.1881,
      "step": 100000
    },
    {
      "epoch": 1.95,
      "grad_norm": 7.865684986114502,
      "learning_rate": 4.3382352941176474e-05,
      "loss": 3.1595,
      "step": 101000
    },
    {
      "epoch": 1.97,
      "grad_norm": 9.688215255737305,
      "learning_rate": 4.2647058823529415e-05,
      "loss": 3.1128,
      "step": 102000
    },
    {
      "epoch": 1.99,
      "grad_norm": 9.678021430969238,
      "learning_rate": 4.1911764705882356e-05,
      "loss": 3.1842,
      "step": 103000
    },
    {
      "epoch": 2.01,
      "grad_norm": 11.641730308532715,
      "learning_rate": 4.11764705882353e-05,
      "loss": 3.1734,
      "step": 104000
    },
    {
      "epoch": 2.03,
      "grad_norm": 7.138424396514893,
      "learning_rate": 4.044117647058824e-05,
      "loss": 3.1004,
      "step": 105000
    },
    {
      "epoch": 2.05,
      "grad_norm": 9.752198219299316,
      "learning_rate": 3.970588235294117e-05,
      "loss": 3.1055,
      "step": 106000
    },
    {
      "epoch": 2.07,
      "grad_norm": 11.197062492370605,
      "learning_rate": 3.897058823529412e-05,
      "loss": 3.0824,
      "step": 107000
    },
    {
      "epoch": 2.09,
      "grad_norm": 9.432929992675781,
      "learning_rate": 3.8235294117647055e-05,
      "loss": 3.1289,
      "step": 108000
    },
    {
      "epoch": 2.11,
      "grad_norm": 9.634468078613281,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 3.1266,
      "step": 109000
    },
    {
      "epoch": 2.12,
      "grad_norm": 9.763446807861328,
      "learning_rate": 3.6764705882352945e-05,
      "loss": 3.07,
      "step": 110000
    },
    {
      "epoch": 2.14,
      "grad_norm": 9.270527839660645,
      "learning_rate": 3.6029411764705886e-05,
      "loss": 3.1038,
      "step": 111000
    },
    {
      "epoch": 2.16,
      "grad_norm": 10.18443775177002,
      "learning_rate": 3.529411764705883e-05,
      "loss": 3.088,
      "step": 112000
    },
    {
      "epoch": 2.18,
      "grad_norm": 15.006034851074219,
      "learning_rate": 3.455882352941177e-05,
      "loss": 3.0705,
      "step": 113000
    },
    {
      "epoch": 2.2,
      "grad_norm": 8.061182975769043,
      "learning_rate": 3.382352941176471e-05,
      "loss": 3.0669,
      "step": 114000
    },
    {
      "epoch": 2.22,
      "grad_norm": 8.676201820373535,
      "learning_rate": 3.308823529411765e-05,
      "loss": 3.0977,
      "step": 115000
    },
    {
      "epoch": 2.24,
      "grad_norm": 8.640947341918945,
      "learning_rate": 3.235294117647059e-05,
      "loss": 3.0672,
      "step": 116000
    },
    {
      "epoch": 2.26,
      "grad_norm": 8.37279224395752,
      "learning_rate": 3.161764705882353e-05,
      "loss": 3.0512,
      "step": 117000
    },
    {
      "epoch": 2.28,
      "grad_norm": 11.125505447387695,
      "learning_rate": 3.0882352941176475e-05,
      "loss": 3.051,
      "step": 118000
    },
    {
      "epoch": 2.3,
      "grad_norm": 7.8079047203063965,
      "learning_rate": 3.0147058823529413e-05,
      "loss": 3.0599,
      "step": 119000
    },
    {
      "epoch": 2.32,
      "grad_norm": 10.24724006652832,
      "learning_rate": 2.9411764705882354e-05,
      "loss": 3.0455,
      "step": 120000
    },
    {
      "epoch": 2.34,
      "grad_norm": 11.889482498168945,
      "learning_rate": 2.8676470588235295e-05,
      "loss": 3.0479,
      "step": 121000
    },
    {
      "epoch": 2.36,
      "grad_norm": 9.233657836914062,
      "learning_rate": 2.7941176470588236e-05,
      "loss": 3.0822,
      "step": 122000
    },
    {
      "epoch": 2.38,
      "grad_norm": 16.01378059387207,
      "learning_rate": 2.7205882352941174e-05,
      "loss": 3.0509,
      "step": 123000
    },
    {
      "epoch": 2.4,
      "grad_norm": 8.904898643493652,
      "learning_rate": 2.647058823529412e-05,
      "loss": 3.0777,
      "step": 124000
    },
    {
      "epoch": 2.41,
      "grad_norm": 11.241424560546875,
      "learning_rate": 2.5735294117647057e-05,
      "loss": 3.0365,
      "step": 125000
    },
    {
      "epoch": 2.43,
      "grad_norm": 8.796116828918457,
      "learning_rate": 2.5e-05,
      "loss": 3.0628,
      "step": 126000
    },
    {
      "epoch": 2.45,
      "grad_norm": 9.860411643981934,
      "learning_rate": 2.4264705882352942e-05,
      "loss": 3.0257,
      "step": 127000
    },
    {
      "epoch": 2.47,
      "grad_norm": 9.723672866821289,
      "learning_rate": 2.3529411764705884e-05,
      "loss": 3.0336,
      "step": 128000
    },
    {
      "epoch": 2.49,
      "grad_norm": 11.798537254333496,
      "learning_rate": 2.2794117647058825e-05,
      "loss": 3.0078,
      "step": 129000
    },
    {
      "epoch": 2.51,
      "grad_norm": 10.427285194396973,
      "learning_rate": 2.2058823529411766e-05,
      "loss": 3.0161,
      "step": 130000
    },
    {
      "epoch": 2.53,
      "grad_norm": 10.898146629333496,
      "learning_rate": 2.1323529411764707e-05,
      "loss": 3.0492,
      "step": 131000
    },
    {
      "epoch": 2.55,
      "grad_norm": 12.747100830078125,
      "learning_rate": 2.058823529411765e-05,
      "loss": 3.0076,
      "step": 132000
    },
    {
      "epoch": 2.57,
      "grad_norm": 10.856698989868164,
      "learning_rate": 1.9852941176470586e-05,
      "loss": 3.0202,
      "step": 133000
    },
    {
      "epoch": 2.59,
      "grad_norm": 11.636751174926758,
      "learning_rate": 1.9117647058823528e-05,
      "loss": 3.0066,
      "step": 134000
    },
    {
      "epoch": 2.61,
      "grad_norm": 8.116796493530273,
      "learning_rate": 1.8382352941176472e-05,
      "loss": 3.0076,
      "step": 135000
    },
    {
      "epoch": 2.63,
      "grad_norm": 10.243114471435547,
      "learning_rate": 1.7647058823529414e-05,
      "loss": 3.0228,
      "step": 136000
    },
    {
      "epoch": 2.65,
      "grad_norm": 7.804482936859131,
      "learning_rate": 1.6911764705882355e-05,
      "loss": 2.9989,
      "step": 137000
    },
    {
      "epoch": 2.67,
      "grad_norm": 12.401259422302246,
      "learning_rate": 1.6176470588235296e-05,
      "loss": 2.9886,
      "step": 138000
    },
    {
      "epoch": 2.68,
      "grad_norm": 9.003000259399414,
      "learning_rate": 1.5441176470588237e-05,
      "loss": 2.9919,
      "step": 139000
    },
    {
      "epoch": 2.7,
      "grad_norm": 10.157318115234375,
      "learning_rate": 1.4705882352941177e-05,
      "loss": 3.0036,
      "step": 140000
    },
    {
      "epoch": 2.72,
      "grad_norm": 10.387412071228027,
      "learning_rate": 1.3970588235294118e-05,
      "loss": 2.976,
      "step": 141000
    },
    {
      "epoch": 2.74,
      "grad_norm": 11.809056282043457,
      "learning_rate": 1.323529411764706e-05,
      "loss": 2.9853,
      "step": 142000
    },
    {
      "epoch": 2.76,
      "grad_norm": 10.666604995727539,
      "learning_rate": 1.25e-05,
      "loss": 3.0239,
      "step": 143000
    },
    {
      "epoch": 2.78,
      "grad_norm": 11.245594024658203,
      "learning_rate": 1.1764705882352942e-05,
      "loss": 3.0032,
      "step": 144000
    },
    {
      "epoch": 2.8,
      "grad_norm": 9.924234390258789,
      "learning_rate": 1.1029411764705883e-05,
      "loss": 3.0026,
      "step": 145000
    },
    {
      "epoch": 2.82,
      "grad_norm": 8.881896018981934,
      "learning_rate": 1.0294117647058824e-05,
      "loss": 2.9937,
      "step": 146000
    },
    {
      "epoch": 2.84,
      "grad_norm": 10.507180213928223,
      "learning_rate": 9.558823529411764e-06,
      "loss": 2.9514,
      "step": 147000
    },
    {
      "epoch": 2.86,
      "grad_norm": 8.399436950683594,
      "learning_rate": 8.823529411764707e-06,
      "loss": 2.9682,
      "step": 148000
    },
    {
      "epoch": 2.88,
      "grad_norm": 9.713330268859863,
      "learning_rate": 8.088235294117648e-06,
      "loss": 3.0068,
      "step": 149000
    },
    {
      "epoch": 2.9,
      "grad_norm": 10.77994441986084,
      "learning_rate": 7.3529411764705884e-06,
      "loss": 2.9663,
      "step": 150000
    },
    {
      "epoch": 2.92,
      "grad_norm": 15.680007934570312,
      "learning_rate": 6.61764705882353e-06,
      "loss": 2.9708,
      "step": 151000
    },
    {
      "epoch": 2.94,
      "grad_norm": 9.687861442565918,
      "learning_rate": 5.882352941176471e-06,
      "loss": 2.947,
      "step": 152000
    },
    {
      "epoch": 2.96,
      "grad_norm": 9.206541061401367,
      "learning_rate": 5.147058823529412e-06,
      "loss": 2.9638,
      "step": 153000
    },
    {
      "epoch": 2.97,
      "grad_norm": 7.6034836769104,
      "learning_rate": 4.411764705882353e-06,
      "loss": 2.9797,
      "step": 154000
    },
    {
      "epoch": 2.99,
      "grad_norm": 8.180992126464844,
      "learning_rate": 3.6764705882352942e-06,
      "loss": 2.9617,
      "step": 155000
    },
    {
      "epoch": 3.01,
      "grad_norm": 9.7806978225708,
      "learning_rate": 2.9411764705882355e-06,
      "loss": 2.9275,
      "step": 156000
    },
    {
      "epoch": 3.03,
      "grad_norm": 10.775460243225098,
      "learning_rate": 2.2058823529411767e-06,
      "loss": 2.9244,
      "step": 157000
    },
    {
      "epoch": 3.05,
      "grad_norm": 9.81534481048584,
      "learning_rate": 1.4705882352941177e-06,
      "loss": 2.9385,
      "step": 158000
    },
    {
      "epoch": 3.07,
      "grad_norm": 10.181273460388184,
      "learning_rate": 7.352941176470589e-07,
      "loss": 2.9168,
      "step": 159000
    },
    {
      "epoch": 3.09,
      "grad_norm": 10.357707023620605,
      "learning_rate": 0.0,
      "loss": 2.9353,
      "step": 160000
    }
  ],
  "logging_steps": 1000,
  "max_steps": 160000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 160000,
  "total_flos": 1.2375487069989888e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
